{"cells":[{"cell_type":"markdown","metadata":{"id":"XslFzHBDU0cw"},"source":["# Korean-Abstractive-Summarization: huggingface_t5-large-korean-text-summary \n","- TEAM: ChatGPT"]},{"cell_type":"markdown","metadata":{"id":"rDVI1hlgU2eI"},"source":["## 1. Install and Setting library "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19926,"status":"ok","timestamp":1684908568371,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"},"user_tz":-540},"id":"aHbEQaqvUuI3","outputId":"4869bb09-344d-4bc2-e0d0-fabf0b542cf8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"]}],"source":["# huggingface transformers library\n","!pip install transformers\n","!pip install --upgrade --pre transformers"]},{"cell_type":"code","source":["!pip install peft "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"taW6waThTltY","executionInfo":{"status":"ok","timestamp":1684908574875,"user_tz":-540,"elapsed":6508,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"}},"outputId":"f99355f2-b72a-4465-a94f-6c5da97bc4ee"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting peft\n","  Downloading peft-0.3.0-py3-none-any.whl (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.0.1+cu118)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.29.2)\n","Collecting accelerate (from peft)\n","  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft) (16.0.5)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.14.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2.27.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft) (2023.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n","Installing collected packages: accelerate, peft\n","Successfully installed accelerate-0.19.0 peft-0.3.0\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5014,"status":"ok","timestamp":1684908579883,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"},"user_tz":-540},"id":"kBYJ2LX3X-EU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"98f30111-9f75-4588-e4a2-93b2832bee3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.19.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n"]}],"source":["!pip install --upgrade accelerate"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":13592,"status":"ok","timestamp":1684908593473,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"},"user_tz":-540},"id":"1IxhIL5tZXMH","colab":{"base_uri":"https://localhost:8080/"},"outputId":"55f44435-7737-4b84-d81b-53de1a4f3ce8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n","Collecting aiohttp (from datasets)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n","Collecting responses<0.19 (from datasets)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n"]}],"source":["# dataset library\n","!pip install datasets"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6005,"status":"ok","timestamp":1684908599473,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"},"user_tz":-540},"id":"bCTyecj9U4NC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9126ffb6-2210-4195-901b-e978c32fc151"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping sentencepiece as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.99\n"]}],"source":["!pip uninstall sentencepiece\n","!pip install sentencepiece"]},{"cell_type":"markdown","metadata":{"id":"blS9zLtFjfFL"},"source":["### 2-3. Split train and test dataset "]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44249,"status":"ok","timestamp":1684908643718,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"},"user_tz":-540},"id":"MTZ6lWolZb_m","outputId":"c2d9f24d-efb0-4726-d84f-ae421344f62a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# 구글 드라이브와 연동합니다. 권한 허용이 필요합니다.\n","from google.colab import drive\n","drive.mount('/content/drive') "]},{"cell_type":"code","execution_count":7,"metadata":{"id":"cHxFpZrWYJOU","executionInfo":{"status":"ok","timestamp":1684908647803,"user_tz":-540,"elapsed":4091,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"}}},"outputs":[],"source":["import pandas as pd\n","\n","train = pd.read_csv('/content/drive/MyDrive/공모전/train_final.csv')"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"jHjeEomHdzFY","executionInfo":{"status":"ok","timestamp":1684908648264,"user_tz":-540,"elapsed":465,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# train과 test 데이터로 분리\n","train_df, test_df = train_test_split(train, test_size=0.2, random_state=24)\n"]},{"cell_type":"markdown","metadata":{"id":"eL6JTj74WdUz"},"source":["## 3. Load model and tokenizer\n","- Train Model: T5\n","    - https://huggingface.co/noahkim/KoT5_news_summarization\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"aGoZdANNXl3h","executionInfo":{"status":"ok","timestamp":1684908660360,"user_tz":-540,"elapsed":1837,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"}}},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer"]},{"cell_type":"markdown","metadata":{"id":"dhtp49sikTl7"},"source":["- fine tuning을 위한 tokenizer 및 model 로드"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":10838,"status":"ok","timestamp":1684908694818,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"},"user_tz":-540},"id":"oZER-nfgVYhf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f38d1b60-d740-4d8a-8a03-1f7925ea48f2"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at /content/drive/MyDrive/공모전/results_t5 were not used when initializing T5ForConditionalGeneration: ['encoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.11.layer.1.EncDecAttention.q.lora_B.default.weight', 'encoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.9.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.8.layer.1.EncDecAttention.q.lora_A.default.weight', 'encoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.4.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.3.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.6.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.3.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.0.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight', 'encoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.10.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.7.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.7.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight', 'encoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.9.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.9.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.9.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight', 'encoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.10.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.4.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.10.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.11.layer.1.EncDecAttention.q.lora_A.default.weight', 'encoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.1.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.11.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.5.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.6.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.1.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.7.layer.1.EncDecAttention.v.lora_B.default.weight', 'decoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.8.layer.1.EncDecAttention.q.lora_B.default.weight', 'encoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.6.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight', 'encoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.8.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight', 'encoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.8.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight', 'encoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight', 'decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight', 'decoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight', 'encoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.10.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.6.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.5.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight', 'decoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.2.layer.1.EncDecAttention.v.lora_B.default.weight', 'encoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight', 'decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight', 'encoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight', 'encoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.11.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight', 'decoder.block.7.layer.1.EncDecAttention.q.lora_A.default.weight', 'encoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight', 'decoder.block.2.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.0.layer.1.EncDecAttention.v.lora_A.default.weight', 'decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight']\n","- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5Tokenizer\n","\n","#tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/공모전/results_t5/checkpoint-900\")\n","#model = AutoModelForSeq2SeqLM.from_pretrained(\"lcw99/t5-base-korean-text-summary\") \n","\n","tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/공모전/results_t5/checkpoint-900\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/drive/MyDrive/공모전/results_t5/checkpoint-900\") # "]},{"cell_type":"markdown","metadata":{"id":"ONSeIs11kMYE"},"source":["- device 설정 (GPU 사용을 권장)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"ymIxZu1gXs2g","executionInfo":{"status":"ok","timestamp":1684908694818,"user_tz":-540,"elapsed":3,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"}}},"outputs":[],"source":["import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"4tOSddRuka3X"},"source":["- 입력과 출력 길이 설정"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"z8UiH63XXnPF","executionInfo":{"status":"ok","timestamp":1684908694819,"user_tz":-540,"elapsed":3,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"}}},"outputs":[],"source":["max_input_length = 1024\n","max_output_length = 102"]},{"cell_type":"markdown","metadata":{"id":"ExBneKzakdQM"},"source":["- 입력 및 출력 토큰화 함수 정의"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"2ZqQnZDGXnHE","executionInfo":{"status":"ok","timestamp":1684908694819,"user_tz":-540,"elapsed":3,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"}}},"outputs":[],"source":["def tokenize(batch):\n","    return tokenizer(batch['text'], padding='max_length', truncation=True, max_length=max_input_length)\n","\n","def summarize(batch):\n","    return tokenizer(batch['summary'], padding='max_length', truncation=True, max_length=max_output_length)"]},{"cell_type":"markdown","metadata":{"id":"NBDYfqCckfNf"},"source":["- 학습 데이터셋 및 데이터로더 생성"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"yxAtkFkuXnBU","executionInfo":{"status":"ok","timestamp":1684908864073,"user_tz":-540,"elapsed":169257,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"}}},"outputs":[],"source":["train_dataset = []\n","for i in range(len(train_df)):\n","    data = {\n","        'id': train_df.iloc[i]['id'],\n","        'input_ids': None,\n","        'attention_mask': None,\n","        'decoder_input_ids': None,\n","        'decoder_attention_mask': None,\n","        'labels': None\n","    }\n","\n","    # 입력과 출력을 토큰화\n","    input = tokenizer.encode_plus(train_df.iloc[i]['text'], max_length=max_input_length, padding='max_length', truncation=True, return_tensors='pt')\n","    output = tokenizer.encode_plus(train_df.iloc[i]['summary'], max_length=max_output_length, padding='max_length', truncation=True, return_tensors='pt')\n","\n","    # 토큰 ID와 어텐션 마스크를 입력 데이터로 저장\n","    data['input_ids'] = input['input_ids'][0]\n","    data['attention_mask'] = input['attention_mask'][0]\n","\n","    # 토큰 ID와 어텐션 마스크를 출력 데이터로 저장\n","    data['decoder_input_ids'] = output['input_ids'][0]\n","    data['decoder_attention_mask'] = output['attention_mask'][0]\n","\n","    # 라벨 데이터로 저장\n","    data['labels'] = output['input_ids'][0].clone().detach()\n","\n","    # 라벨에서 PAD 토큰의 위치를 마스크하여 불필요한 손실 계산 방지\n","    data['labels'][data['labels'] == tokenizer.pad_token_id] = -100\n","\n","    train_dataset.append(data)"]},{"cell_type":"markdown","metadata":{"id":"gitJNN3nklTD"},"source":["- 테스트 데이터셋 및 데이터로더 생성"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"qZhmdL5Ud55e","executionInfo":{"status":"ok","timestamp":1684908909054,"user_tz":-540,"elapsed":44984,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"}}},"outputs":[],"source":["test_dataset = []\n","for i in range(len(test_df)):\n","    data = {\n","        'id': test_df.iloc[i]['id'],\n","        'input_ids': None,\n","        'attention_mask': None,\n","        'decoder_input_ids': None,\n","        'decoder_attention_mask': None,\n","        'labels': None\n","    }\n","\n","    # 입력과 출력을 토큰화\n","    input = tokenizer.encode_plus(test_df.iloc[i]['text'], max_length=max_input_length, padding='max_length', truncation=True, return_tensors='pt')\n","    output = tokenizer.encode_plus(test_df.iloc[i]['summary'], max_length=max_output_length, padding='max_length', truncation=True, return_tensors='pt')\n","\n","    # 토큰 ID와 어텐션 마스크를 입력 데이터로 저장\n","    data['input_ids'] = input['input_ids'][0]\n","    data['attention_mask'] = input['attention_mask'][0]\n","\n","    # 토큰 ID와 어텐션 마스크를 출력 데이터로 저장\n","    data['decoder_input_ids'] = output['input_ids'][0]\n","    data['decoder_attention_mask'] = output['attention_mask'][0]\n","\n","    # 라벨 데이터로 저장\n","    data['labels'] = output['input_ids'][0].clone().detach()\n","\n","    # 라벨에서 PAD 토큰의 위치를 마스크하여 불필요한 손실 계산 방지\n","    data['labels'][data['labels'] == tokenizer.pad_token_id] = -100\n","\n","    test_dataset.append(data)"]},{"cell_type":"markdown","metadata":{"id":"P6VkALcKkn0v"},"source":["- 데이터 수집기 및 학습/테스트 데이터셋 로더 생성"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"ZXCabD57YmN7","executionInfo":{"status":"ok","timestamp":1684908909055,"user_tz":-540,"elapsed":12,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"}}},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,model=model)\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=data_collator)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=data_collator)"]},{"cell_type":"markdown","source":["# PEFT"],"metadata":{"id":"JZ-Tg-g0TEbr"}},{"cell_type":"code","source":["from peft import get_peft_model, PeftModel, TaskType, LoraConfig\n","\n","PEFT_CONFIG = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n","PEFT_MODEL = get_peft_model(model, PEFT_CONFIG)\n","PEFT_MODEL.to('cuda')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VplablMSTAWV","executionInfo":{"status":"ok","timestamp":1684908914362,"user_tz":-540,"elapsed":5318,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"}},"outputId":"3f8192a5-c961-4512-9aca-fd801af7115a"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PeftModelForSeq2SeqLM(\n","  (base_model): LoraModel(\n","    (model): T5ForConditionalGeneration(\n","      (shared): Embedding(50358, 768)\n","      (encoder): T5Stack(\n","        (embed_tokens): Embedding(50358, 768)\n","        (block): ModuleList(\n","          (0): T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                  (relative_attention_bias): Embedding(32, 12)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","          (1-11): 11 x T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","        )\n","        (final_layer_norm): T5LayerNorm()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (decoder): T5Stack(\n","        (embed_tokens): Embedding(50358, 768)\n","        (block): ModuleList(\n","          (0): T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                  (relative_attention_bias): Embedding(32, 12)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerCrossAttention(\n","                (EncDecAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (2): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","          (1-11): 11 x T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerCrossAttention(\n","                (EncDecAttention): T5Attention(\n","                  (q): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (k): Linear(in_features=768, out_features=768, bias=False)\n","                  (v): Linear(\n","                    in_features=768, out_features=768, bias=False\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=768, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=768, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                  )\n","                  (o): Linear(in_features=768, out_features=768, bias=False)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (2): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n","                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","        )\n","        (final_layer_norm): T5LayerNorm()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (lm_head): Linear(in_features=768, out_features=50358, bias=False)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"dd0rtskClOFm"},"source":["## 5. Train model"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"ODaPuid-SjOU","executionInfo":{"status":"ok","timestamp":1684908914362,"user_tz":-540,"elapsed":8,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"}}},"outputs":[],"source":["from transformers import EarlyStoppingCallback\n","\n","# EarlyStoppingCallback 초기화\n","early_stopping = EarlyStoppingCallback(early_stopping_patience=100, early_stopping_threshold=0.01)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"UYlojk1DWHGm","colab":{"base_uri":"https://localhost:8080/","height":845},"executionInfo":{"status":"error","timestamp":1684924192953,"user_tz":-540,"elapsed":15278598,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"}},"outputId":"9a8655d2-92d0-4de0-9ac6-ebf8033d8e0b"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='919' max='56335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  919/56335 4:14:32 < 256:22:25, 0.06 it/s, Epoch 0.08/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.009500</td>\n","      <td>0.742257</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.848400</td>\n","      <td>0.723271</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.801700</td>\n","      <td>0.709641</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.773200</td>\n","      <td>0.701328</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.720100</td>\n","      <td>0.693906</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.769600</td>\n","      <td>0.680251</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.729500</td>\n","      <td>0.672478</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.736900</td>\n","      <td>0.665179</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.721200</td>\n","      <td>0.662521</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n","\u001b[31m│\u001b[0m in \u001b[92m<cell line: 40>\u001b[0m:\u001b[94m40\u001b[0m                                                                            \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1664\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1661 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1662 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1663 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1664 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1665 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1666 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1667 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1945\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m     \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1942 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1943 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1944 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m is_torch_tpu_available()                                      \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1945 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[95mand\u001b[0m (torch.isnan(tr_loss_step) \u001b[95mor\u001b[0m torch.isinf(tr_loss_step))          \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1946 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m):                                                                        \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1947 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# if loss is nan or inf simply add the average of previous logged lo\u001b[0m  \u001b[31m│\u001b[0m\n","\u001b[31m│\u001b[0m   \u001b[2m1948 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mtr_loss += tr_loss / (\u001b[94m1\u001b[0m + \u001b[96mself\u001b[0m.state.global_step - \u001b[96mself\u001b[0m._globalstep_  \u001b[31m│\u001b[0m\n","\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n","\u001b[1;91mKeyboardInterrupt\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 40&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">40</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1664</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1661 │   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1662 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1663 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1664 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1665 │   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1666 │   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1667 │   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1945</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1942 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1943 │   │   │   │   │   </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1944 │   │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> is_torch_tpu_available()                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1945 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> (torch.isnan(tr_loss_step) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> torch.isinf(tr_loss_step))          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1946 │   │   │   │   </span>):                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1947 │   │   │   │   │   # if loss is nan or inf simply add the average of previous logged lo</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1948 │   │   │   │   │   </span>tr_loss += tr_loss / (<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.global_step - <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._globalstep_  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n","<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n","<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n","</pre>\n"]},"metadata":{}}],"source":["from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","from transformers import AdamW\n","\n","# 파인튜닝을 위한 argument 설정\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir='/content/drive/MyDrive/공모전/results_t5',\n","    evaluation_strategy = \"steps\",\n","    eval_steps = 100, # Number of update steps between two evaluations.\n","    save_steps= 100, # after # steps model is saved \n","    warmup_steps= 200,# number of warmup steps for learning rate scheduler\n","    num_train_epochs = 5,\n","    learning_rate = 2e-5,\n","    per_device_train_batch_size = 2,\n","    per_device_eval_batch_size = 2,\n","    weight_decay = 0.01,\n","    push_to_hub=False,\n","    logging_dir='/content/drive/MyDrive/공모전/logs_t5',\n","    logging_steps=100,\n","    overwrite_output_dir=True,\n","    predict_with_generate=True,\n","    load_best_model_at_end = True,\n","    gradient_accumulation_steps=2,\n","    seed=42\n",")\n","optimizer = AdamW(model.parameters(), lr=training_args.learning_rate, eps=1e-8)\n","\n","# trainer 객체 생성\n","trainer = Seq2SeqTrainer(\n","    model=PEFT_MODEL,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    callbacks=[early_stopping],\n","    optimizers=(optimizer, None)\n",")\n","\n","# 파인튜닝 시작\n","trainer.train()"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"B0oSeqnypRyq","executionInfo":{"status":"ok","timestamp":1684924199263,"user_tz":-540,"elapsed":3743,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"}}},"outputs":[],"source":["model.save_pretrained(\"/content/drive/MyDrive/공모전/results_t5\") # 모델 저장장"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"_KMRw4Cbbk9r","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"ok","timestamp":1684925770308,"user_tz":-540,"elapsed":1571049,"user":{"displayName":"캡스톤IVE","userId":"00895441837207177701"}},"outputId":"e1f7e214-66a4-4e0f-c58f-3d22a1c65351"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='919' max='56335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  919/56335 4:14:32 < 256:22:25, 0.06 it/s, Epoch 0.08/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>100</td>\n","      <td>1.009500</td>\n","      <td>0.742257</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.848400</td>\n","      <td>0.723271</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.801700</td>\n","      <td>0.709641</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.773200</td>\n","      <td>0.701328</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.720100</td>\n","      <td>0.693906</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.769600</td>\n","      <td>0.680251</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.729500</td>\n","      <td>0.672478</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.736900</td>\n","      <td>0.665179</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.721200</td>\n","      <td>0.662521</td>\n","    </tr>\n","    <tr>\n","      <td>918</td>\n","      <td>0.721200</td>\n","      <td>0.659639</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.6596390604972839}"]},"metadata":{},"execution_count":24}],"source":["trainer.evaluate()"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}